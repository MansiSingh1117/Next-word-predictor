{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53d8555d",
   "metadata": {},
   "source": [
    "# Next Word Prediction with NLP and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abe9a3c",
   "metadata": {},
   "source": [
    "- The model will consider the last word of a particular sentence and predict the next possible word. \n",
    "- We will be using methods of natural language processing, language modeling, and deep learning. \n",
    "- We will start by analyzing the data followed by the pre-processing of the data. \n",
    "- We will then tokenize this data and finally build the deep learning model. The deep learning model will be built using LSTM’s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce5b2ff",
   "metadata": {},
   "source": [
    "# Inserting required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf45dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense,Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fc3786",
   "metadata": {},
   "source": [
    "<h3>1.PRE - PROCESSING THE DATASET</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d135632",
   "metadata": {},
   "source": [
    "- Next step of our cleaning process involves replacing all the unnecessary extra new lines, the carriage return, and the Unicode character. \n",
    "- Finally, we will make sure we have only unique words. \n",
    "- We will consider each word only once and remove any additional repetitions. \n",
    "- This will help the model train better avoiding extra confusion due to the repetition of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e73965e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Line:  One morning, when Gregor Samsa woke from troubled dreams, he found\n",
      "\n",
      "The Last Line:  first to get up and stretch out her young body.\n"
     ]
    }
   ],
   "source": [
    "file = open(\"metamorphosis_clean.txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "    \n",
    "print(\"The First Line: \", lines[0])\n",
    "print(\"The Last Line: \", lines[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a77cd8",
   "metadata": {},
   "source": [
    "### Cleaning the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52836d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.  He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off any moment.  His many legs, pitifully thin compared with the size of the rest of him, waved about helplessly as he looked.  \"What\\'s happened to me?\" he'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"\"\n",
    "\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "    \n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "\n",
    "#printing the data\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d447825c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning  when Gregor Samsa woke from troubled dreams  he found himself transformed in his bed into a horrible vermin   He lay on his armour like back  and if he lifted his head a little he could see his brown belly  slightly domed and divided by arches into stiff sections   The bedding was hardly able to cover it and seemed ready to slide off any moment   His many legs  pitifully thin compared with the size of the rest of him  waved about helplessly as he looked    What s happened to me   he'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "#removes all the punct\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "new_data = data.translate(translator)\n",
    "\n",
    "new_data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4253478a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on armour-like back, and if lifted head little could see brown belly, slightly domed divided by arches stiff sections. The bedding was hardly able to cover it seemed ready slide off any moment. His many legs, pitifully thin compared with the size of rest him, waved about helplessly as looked. \"What\\'s happened me?\" thought. It wasn\\'t dream. room, proper human room altho'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removes all the repedted words in the sentence\n",
    "z = []\n",
    "\n",
    "for i in data.split():\n",
    "    if i not in z:\n",
    "        z.append(i)\n",
    "        \n",
    "data = ' '.join(z)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84757dd",
   "metadata": {},
   "source": [
    "### Tokenization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da32177",
   "metadata": {},
   "source": [
    "- Tokenization refers to splitting bigger text data, essays, or corpus’s into smaller segments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1602690a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 53, 293, 2, 18, 729, 135, 730, 294, 8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function.\n",
    "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1618af3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2617\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a56f722a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  3889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 17,  53],\n",
       "       [ 53, 293],\n",
       "       [293,   2],\n",
       "       [  2,  18],\n",
       "       [ 18, 729],\n",
       "       [729, 135],\n",
       "       [135, 730],\n",
       "       [730, 294],\n",
       "       [294,   8],\n",
       "       [  8, 731]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(1, len(sequence_data)):\n",
    "    words = sequence_data[i-1:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f1265b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0])\n",
    "    y.append(i[1])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4144a587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data is:  [ 17  53 293   2  18]\n",
      "The responses are:  [ 53 293   2  18 729]\n"
     ]
    }
   ],
   "source": [
    "print(\"The Data is: \", X[:5])\n",
    "print(\"The responses are: \", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c060241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4f0916",
   "metadata": {},
   "source": [
    "# CREATING THE MODEL:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ec953d",
   "metadata": {},
   "source": [
    "- We will be building a sequential model. \n",
    "- We will then create an embedding layer and specify the input dimensions and output dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d7e4c1",
   "metadata": {},
   "source": [
    "##### An LSTM layer learns long-term dependencies between time steps in time series and sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3850c9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "class attention(Layer):\n",
    "    def init(self):\n",
    "        super(attention,self).__init__()\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name='att_weight',shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name='att_bias',shape=(input_shape[-2],1),initializer=\"zeros\")        \n",
    "        super(attention, self).build(input_shape)\n",
    "    def call(self,x):\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x*a\n",
    "        return K.sum(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f3fd7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(Bidirectional(LSTM(1000, return_sequences=True)))\n",
    "model.add(attention())\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "450181b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1, 10)             26170     \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 1, 2000)          8088000   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " attention (attention)       (None, 2000)              2001      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              2001000   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2617)              2619617   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,736,788\n",
      "Trainable params: 12,736,788\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b7389b",
   "metadata": {},
   "source": [
    "<h4>MODEL PLOT</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "698544ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pydot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84c52aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "keras.utils.plot_model(model, to_file='model.png', show_layer_names=True)"
   ]
  },
  {
   "attachments": {
    "model.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "220af379",
   "metadata": {},
   "source": [
    "![model.png](attachment:model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31e3f0f",
   "metadata": {},
   "source": [
    "<h4>Callbacks:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f1b4d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword1'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db8a2e8",
   "metadata": {},
   "source": [
    "<h4>Compile the model: </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de1db91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf09c357",
   "metadata": {},
   "source": [
    "<h4>Fit the model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c2e9616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.8755 - accuracy: 0.0013\n",
      "Epoch 1: loss improved from inf to 7.87549, saving model to nextword1.h5\n",
      "61/61 [==============================] - 31s 257ms/step - loss: 7.8755 - accuracy: 0.0013 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.8625 - accuracy: 0.0028\n",
      "Epoch 2: loss improved from 7.87549 to 7.86252, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 250ms/step - loss: 7.8625 - accuracy: 0.0028 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.8257 - accuracy: 0.0026\n",
      "Epoch 3: loss improved from 7.86252 to 7.82573, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 267ms/step - loss: 7.8257 - accuracy: 0.0026 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.6161 - accuracy: 0.0049\n",
      "Epoch 4: loss improved from 7.82573 to 7.61613, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 248ms/step - loss: 7.6161 - accuracy: 0.0049 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.3696 - accuracy: 0.0018\n",
      "Epoch 5: loss improved from 7.61613 to 7.36959, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 261ms/step - loss: 7.3696 - accuracy: 0.0018 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.0578 - accuracy: 0.0046\n",
      "Epoch 6: loss improved from 7.36959 to 7.05781, saving model to nextword1.h5\n",
      "61/61 [==============================] - 19s 320ms/step - loss: 7.0578 - accuracy: 0.0046 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.7354 - accuracy: 0.0039\n",
      "Epoch 7: loss improved from 7.05781 to 6.73543, saving model to nextword1.h5\n",
      "61/61 [==============================] - 18s 301ms/step - loss: 6.7354 - accuracy: 0.0039 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.4336 - accuracy: 0.0062\n",
      "Epoch 8: loss improved from 6.73543 to 6.43358, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 253ms/step - loss: 6.4336 - accuracy: 0.0062 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.1309 - accuracy: 0.0105\n",
      "Epoch 9: loss improved from 6.43358 to 6.13090, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 226ms/step - loss: 6.1309 - accuracy: 0.0105 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.7268 - accuracy: 0.0131\n",
      "Epoch 10: loss improved from 6.13090 to 5.72678, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 228ms/step - loss: 5.7268 - accuracy: 0.0131 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.1539 - accuracy: 0.0350\n",
      "Epoch 11: loss improved from 5.72678 to 5.15387, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 257ms/step - loss: 5.1539 - accuracy: 0.0350 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.4094 - accuracy: 0.0751\n",
      "Epoch 12: loss improved from 5.15387 to 4.40945, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 269ms/step - loss: 4.4094 - accuracy: 0.0751 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.6736 - accuracy: 0.1540\n",
      "Epoch 13: loss improved from 4.40945 to 3.67356, saving model to nextword1.h5\n",
      "61/61 [==============================] - 20s 333ms/step - loss: 3.6736 - accuracy: 0.1540 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.0313 - accuracy: 0.2664\n",
      "Epoch 14: loss improved from 3.67356 to 3.03130, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 265ms/step - loss: 3.0313 - accuracy: 0.2664 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.5207 - accuracy: 0.3649\n",
      "Epoch 15: loss improved from 3.03130 to 2.52068, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 189ms/step - loss: 2.5207 - accuracy: 0.3649 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.1163 - accuracy: 0.4520\n",
      "Epoch 16: loss improved from 2.52068 to 2.11633, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 232ms/step - loss: 2.1163 - accuracy: 0.4520 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.8189 - accuracy: 0.5210\n",
      "Epoch 17: loss improved from 2.11633 to 1.81893, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 287ms/step - loss: 1.8189 - accuracy: 0.5210 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.6144 - accuracy: 0.5469\n",
      "Epoch 18: loss improved from 1.81893 to 1.61445, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 286ms/step - loss: 1.6144 - accuracy: 0.5469 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4476 - accuracy: 0.5719\n",
      "Epoch 19: loss improved from 1.61445 to 1.44763, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 252ms/step - loss: 1.4476 - accuracy: 0.5719 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.3336 - accuracy: 0.5760\n",
      "Epoch 20: loss improved from 1.44763 to 1.33361, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 228ms/step - loss: 1.3336 - accuracy: 0.5760 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2554 - accuracy: 0.5729\n",
      "Epoch 21: loss improved from 1.33361 to 1.25536, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 225ms/step - loss: 1.2554 - accuracy: 0.5729 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2072 - accuracy: 0.5762\n",
      "Epoch 22: loss improved from 1.25536 to 1.20722, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 230ms/step - loss: 1.2072 - accuracy: 0.5762 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1518 - accuracy: 0.5744\n",
      "Epoch 23: loss improved from 1.20722 to 1.15182, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 249ms/step - loss: 1.1518 - accuracy: 0.5744 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1053 - accuracy: 0.5765\n",
      "Epoch 24: loss improved from 1.15182 to 1.10533, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 274ms/step - loss: 1.1053 - accuracy: 0.5765 - lr: 0.0010\n",
      "Epoch 25/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0677 - accuracy: 0.5791\n",
      "Epoch 25: loss improved from 1.10533 to 1.06772, saving model to nextword1.h5\n",
      "61/61 [==============================] - 18s 292ms/step - loss: 1.0677 - accuracy: 0.5791 - lr: 0.0010\n",
      "Epoch 26/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0391 - accuracy: 0.5834\n",
      "Epoch 26: loss improved from 1.06772 to 1.03908, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 258ms/step - loss: 1.0391 - accuracy: 0.5834 - lr: 0.0010\n",
      "Epoch 27/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0211 - accuracy: 0.5750\n",
      "Epoch 27: loss improved from 1.03908 to 1.02109, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 225ms/step - loss: 1.0211 - accuracy: 0.5750 - lr: 0.0010\n",
      "Epoch 28/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0012 - accuracy: 0.5760\n",
      "Epoch 28: loss improved from 1.02109 to 1.00120, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 223ms/step - loss: 1.0012 - accuracy: 0.5760 - lr: 0.0010\n",
      "Epoch 29/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.9714 - accuracy: 0.5811\n",
      "Epoch 29: loss improved from 1.00120 to 0.97144, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 227ms/step - loss: 0.9714 - accuracy: 0.5811 - lr: 0.0010\n",
      "Epoch 30/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.9571 - accuracy: 0.5773\n",
      "Epoch 30: loss improved from 0.97144 to 0.95714, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 272ms/step - loss: 0.9571 - accuracy: 0.5773 - lr: 0.0010\n",
      "Epoch 31/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.9405 - accuracy: 0.5778\n",
      "Epoch 31: loss improved from 0.95714 to 0.94045, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 285ms/step - loss: 0.9405 - accuracy: 0.5778 - lr: 0.0010\n",
      "Epoch 32/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.9271 - accuracy: 0.5786\n",
      "Epoch 32: loss improved from 0.94045 to 0.92709, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 258ms/step - loss: 0.9271 - accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 33/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.9144 - accuracy: 0.5827\n",
      "Epoch 33: loss improved from 0.92709 to 0.91443, saving model to nextword1.h5\n",
      "61/61 [==============================] - 13s 221ms/step - loss: 0.9144 - accuracy: 0.5827 - lr: 0.0010\n",
      "Epoch 34/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.9104 - accuracy: 0.5724\n",
      "Epoch 34: loss improved from 0.91443 to 0.91039, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 225ms/step - loss: 0.9104 - accuracy: 0.5724 - lr: 0.0010\n",
      "Epoch 35/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8975 - accuracy: 0.5750\n",
      "Epoch 35: loss improved from 0.91039 to 0.89746, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 226ms/step - loss: 0.8975 - accuracy: 0.5750 - lr: 0.0010\n",
      "Epoch 36/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8865 - accuracy: 0.5798\n",
      "Epoch 36: loss improved from 0.89746 to 0.88651, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 237ms/step - loss: 0.8865 - accuracy: 0.5798 - lr: 0.0010\n",
      "Epoch 37/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8765 - accuracy: 0.5845\n",
      "Epoch 37: loss improved from 0.88651 to 0.87654, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 255ms/step - loss: 0.8765 - accuracy: 0.5845 - lr: 0.0010\n",
      "Epoch 38/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8699 - accuracy: 0.5734\n",
      "Epoch 38: loss improved from 0.87654 to 0.86993, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 257ms/step - loss: 0.8699 - accuracy: 0.5734 - lr: 0.0010\n",
      "Epoch 39/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8618 - accuracy: 0.5778\n",
      "Epoch 39: loss improved from 0.86993 to 0.86176, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 288ms/step - loss: 0.8618 - accuracy: 0.5778 - lr: 0.0010\n",
      "Epoch 40/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8570 - accuracy: 0.5809\n",
      "Epoch 40: loss improved from 0.86176 to 0.85698, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 245ms/step - loss: 0.8570 - accuracy: 0.5809 - lr: 0.0010\n",
      "Epoch 41/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8441 - accuracy: 0.5732\n",
      "Epoch 41: loss improved from 0.85698 to 0.84409, saving model to nextword1.h5\n",
      "61/61 [==============================] - 13s 209ms/step - loss: 0.8441 - accuracy: 0.5732 - lr: 0.0010\n",
      "Epoch 42/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8419 - accuracy: 0.5811\n",
      "Epoch 42: loss improved from 0.84409 to 0.84187, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 226ms/step - loss: 0.8419 - accuracy: 0.5811 - lr: 0.0010\n",
      "Epoch 43/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8382 - accuracy: 0.5765\n",
      "Epoch 43: loss improved from 0.84187 to 0.83822, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 230ms/step - loss: 0.8382 - accuracy: 0.5765 - lr: 0.0010\n",
      "Epoch 44/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8299 - accuracy: 0.5780\n",
      "Epoch 44: loss improved from 0.83822 to 0.82992, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 252ms/step - loss: 0.8299 - accuracy: 0.5780 - lr: 0.0010\n",
      "Epoch 45/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8204 - accuracy: 0.5765\n",
      "Epoch 45: loss improved from 0.82992 to 0.82045, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 245ms/step - loss: 0.8204 - accuracy: 0.5765 - lr: 0.0010\n",
      "Epoch 46/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8192 - accuracy: 0.5791\n",
      "Epoch 46: loss improved from 0.82045 to 0.81924, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 259ms/step - loss: 0.8192 - accuracy: 0.5791 - lr: 0.0010\n",
      "Epoch 47/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8118 - accuracy: 0.5796\n",
      "Epoch 47: loss improved from 0.81924 to 0.81180, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 286ms/step - loss: 0.8118 - accuracy: 0.5796 - lr: 0.0010\n",
      "Epoch 48/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8052 - accuracy: 0.5747\n",
      "Epoch 48: loss improved from 0.81180 to 0.80521, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 233ms/step - loss: 0.8052 - accuracy: 0.5747 - lr: 0.0010\n",
      "Epoch 49/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8051 - accuracy: 0.5798\n",
      "Epoch 49: loss improved from 0.80521 to 0.80510, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 228ms/step - loss: 0.8051 - accuracy: 0.5798 - lr: 0.0010\n",
      "Epoch 50/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8022 - accuracy: 0.5724\n",
      "Epoch 50: loss improved from 0.80510 to 0.80223, saving model to nextword1.h5\n",
      "61/61 [==============================] - 13s 217ms/step - loss: 0.8022 - accuracy: 0.5724 - lr: 0.0010\n",
      "Epoch 51/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7979 - accuracy: 0.5801\n",
      "Epoch 51: loss improved from 0.80223 to 0.79793, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 246ms/step - loss: 0.7979 - accuracy: 0.5801 - lr: 0.0010\n",
      "Epoch 52/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7948 - accuracy: 0.5786\n",
      "Epoch 52: loss improved from 0.79793 to 0.79475, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 245ms/step - loss: 0.7948 - accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 53/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7891 - accuracy: 0.5822\n",
      "Epoch 53: loss improved from 0.79475 to 0.78907, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 254ms/step - loss: 0.7891 - accuracy: 0.5822 - lr: 0.0010\n",
      "Epoch 54/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7891 - accuracy: 0.5744\n",
      "Epoch 54: loss did not improve from 0.78907\n",
      "61/61 [==============================] - 13s 211ms/step - loss: 0.7891 - accuracy: 0.5744 - lr: 0.0010\n",
      "Epoch 55/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7804 - accuracy: 0.5783\n",
      "Epoch 55: loss improved from 0.78907 to 0.78039, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 228ms/step - loss: 0.7804 - accuracy: 0.5783 - lr: 0.0010\n",
      "Epoch 56/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7773 - accuracy: 0.5755\n",
      "Epoch 56: loss improved from 0.78039 to 0.77728, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 245ms/step - loss: 0.7773 - accuracy: 0.5755 - lr: 0.0010\n",
      "Epoch 57/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7774 - accuracy: 0.5747\n",
      "Epoch 57: loss did not improve from 0.77728\n",
      "61/61 [==============================] - 12s 192ms/step - loss: 0.7774 - accuracy: 0.5747 - lr: 0.0010\n",
      "Epoch 58/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7744 - accuracy: 0.5786\n",
      "Epoch 58: loss improved from 0.77728 to 0.77437, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 230ms/step - loss: 0.7744 - accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 59/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7725 - accuracy: 0.5768\n",
      "Epoch 59: loss improved from 0.77437 to 0.77253, saving model to nextword1.h5\n",
      "61/61 [==============================] - 18s 289ms/step - loss: 0.7725 - accuracy: 0.5768 - lr: 0.0010\n",
      "Epoch 60/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7661 - accuracy: 0.5732\n",
      "Epoch 60: loss improved from 0.77253 to 0.76610, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 252ms/step - loss: 0.7661 - accuracy: 0.5732 - lr: 0.0010\n",
      "Epoch 61/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7695 - accuracy: 0.5696\n",
      "Epoch 61: loss did not improve from 0.76610\n",
      "61/61 [==============================] - 13s 208ms/step - loss: 0.7695 - accuracy: 0.5696 - lr: 0.0010\n",
      "Epoch 62/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7623 - accuracy: 0.5760\n",
      "Epoch 62: loss improved from 0.76610 to 0.76231, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 253ms/step - loss: 0.7623 - accuracy: 0.5760 - lr: 0.0010\n",
      "Epoch 63/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7593 - accuracy: 0.5716\n",
      "Epoch 63: loss improved from 0.76231 to 0.75931, saving model to nextword1.h5\n",
      "61/61 [==============================] - 13s 209ms/step - loss: 0.7593 - accuracy: 0.5716 - lr: 0.0010\n",
      "Epoch 64/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7579 - accuracy: 0.5760\n",
      "Epoch 64: loss improved from 0.75931 to 0.75787, saving model to nextword1.h5\n",
      "61/61 [==============================] - 13s 213ms/step - loss: 0.7579 - accuracy: 0.5760 - lr: 0.0010\n",
      "Epoch 65/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7548 - accuracy: 0.5788\n",
      "Epoch 65: loss improved from 0.75787 to 0.75482, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 244ms/step - loss: 0.7548 - accuracy: 0.5788 - lr: 0.0010\n",
      "Epoch 66/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7553 - accuracy: 0.5768\n",
      "Epoch 66: loss did not improve from 0.75482\n",
      "61/61 [==============================] - 13s 215ms/step - loss: 0.7553 - accuracy: 0.5768 - lr: 0.0010\n",
      "Epoch 67/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7503 - accuracy: 0.5783\n",
      "Epoch 67: loss improved from 0.75482 to 0.75033, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 239ms/step - loss: 0.7503 - accuracy: 0.5783 - lr: 0.0010\n",
      "Epoch 68/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7523 - accuracy: 0.5734\n",
      "Epoch 68: loss did not improve from 0.75033\n",
      "61/61 [==============================] - 13s 219ms/step - loss: 0.7523 - accuracy: 0.5734 - lr: 0.0010\n",
      "Epoch 69/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7453 - accuracy: 0.5786\n",
      "Epoch 69: loss improved from 0.75033 to 0.74532, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 227ms/step - loss: 0.7453 - accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 70/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7477 - accuracy: 0.5778\n",
      "Epoch 70: loss did not improve from 0.74532\n",
      "61/61 [==============================] - 12s 201ms/step - loss: 0.7477 - accuracy: 0.5778 - lr: 0.0010\n",
      "Epoch 71/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7447 - accuracy: 0.5765\n",
      "Epoch 71: loss improved from 0.74532 to 0.74471, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 246ms/step - loss: 0.7447 - accuracy: 0.5765 - lr: 0.0010\n",
      "Epoch 72/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7425 - accuracy: 0.5770\n",
      "Epoch 72: loss improved from 0.74471 to 0.74248, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 233ms/step - loss: 0.7425 - accuracy: 0.5770 - lr: 0.0010\n",
      "Epoch 73/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7381 - accuracy: 0.5778\n",
      "Epoch 73: loss improved from 0.74248 to 0.73812, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 259ms/step - loss: 0.7381 - accuracy: 0.5778 - lr: 0.0010\n",
      "Epoch 74/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7393 - accuracy: 0.5737\n",
      "Epoch 74: loss did not improve from 0.73812\n",
      "61/61 [==============================] - 13s 209ms/step - loss: 0.7393 - accuracy: 0.5737 - lr: 0.0010\n",
      "Epoch 75/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7374 - accuracy: 0.5832\n",
      "Epoch 75: loss improved from 0.73812 to 0.73743, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 233ms/step - loss: 0.7374 - accuracy: 0.5832 - lr: 0.0010\n",
      "Epoch 76/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7353 - accuracy: 0.5798\n",
      "Epoch 76: loss improved from 0.73743 to 0.73527, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 235ms/step - loss: 0.7353 - accuracy: 0.5798 - lr: 0.0010\n",
      "Epoch 77/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7319 - accuracy: 0.5675\n",
      "Epoch 77: loss improved from 0.73527 to 0.73185, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 226ms/step - loss: 0.7319 - accuracy: 0.5675 - lr: 0.0010\n",
      "Epoch 78/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7340 - accuracy: 0.5762\n",
      "Epoch 78: loss did not improve from 0.73185\n",
      "61/61 [==============================] - 11s 183ms/step - loss: 0.7340 - accuracy: 0.5762 - lr: 0.0010\n",
      "Epoch 79/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7300 - accuracy: 0.5778\n",
      "Epoch 79: loss improved from 0.73185 to 0.73000, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 279ms/step - loss: 0.7300 - accuracy: 0.5778 - lr: 0.0010\n",
      "Epoch 80/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7278 - accuracy: 0.5796\n",
      "Epoch 80: loss improved from 0.73000 to 0.72775, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 246ms/step - loss: 0.7278 - accuracy: 0.5796 - lr: 0.0010\n",
      "Epoch 81/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7242 - accuracy: 0.5747\n",
      "Epoch 81: loss improved from 0.72775 to 0.72416, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 238ms/step - loss: 0.7242 - accuracy: 0.5747 - lr: 0.0010\n",
      "Epoch 82/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7257 - accuracy: 0.5775\n",
      "Epoch 82: loss did not improve from 0.72416\n",
      "61/61 [==============================] - 11s 187ms/step - loss: 0.7257 - accuracy: 0.5775 - lr: 0.0010\n",
      "Epoch 83/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7235 - accuracy: 0.5786\n",
      "Epoch 83: loss improved from 0.72416 to 0.72350, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 252ms/step - loss: 0.7235 - accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 84/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7174 - accuracy: 0.5824\n",
      "Epoch 84: loss improved from 0.72350 to 0.71737, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 260ms/step - loss: 0.7174 - accuracy: 0.5824 - lr: 0.0010\n",
      "Epoch 85/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7166 - accuracy: 0.5775\n",
      "Epoch 85: loss improved from 0.71737 to 0.71664, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 274ms/step - loss: 0.7166 - accuracy: 0.5775 - lr: 0.0010\n",
      "Epoch 86/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7227 - accuracy: 0.5721\n",
      "Epoch 86: loss did not improve from 0.71664\n",
      "61/61 [==============================] - 13s 219ms/step - loss: 0.7227 - accuracy: 0.5721 - lr: 0.0010\n",
      "Epoch 87/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7164 - accuracy: 0.5701\n",
      "Epoch 87: loss improved from 0.71664 to 0.71636, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 234ms/step - loss: 0.7164 - accuracy: 0.5701 - lr: 0.0010\n",
      "Epoch 88/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7165 - accuracy: 0.5806\n",
      "Epoch 88: loss did not improve from 0.71636\n",
      "61/61 [==============================] - 13s 218ms/step - loss: 0.7165 - accuracy: 0.5806 - lr: 0.0010\n",
      "Epoch 89/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7146 - accuracy: 0.5690\n",
      "Epoch 89: loss improved from 0.71636 to 0.71463, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 272ms/step - loss: 0.7146 - accuracy: 0.5690 - lr: 0.0010\n",
      "Epoch 90/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7123 - accuracy: 0.5801\n",
      "Epoch 90: loss improved from 0.71463 to 0.71233, saving model to nextword1.h5\n",
      "61/61 [==============================] - 18s 289ms/step - loss: 0.7123 - accuracy: 0.5801 - lr: 0.0010\n",
      "Epoch 91/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7146 - accuracy: 0.5775\n",
      "Epoch 91: loss did not improve from 0.71233\n",
      "61/61 [==============================] - 14s 227ms/step - loss: 0.7146 - accuracy: 0.5775 - lr: 0.0010\n",
      "Epoch 92/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7157 - accuracy: 0.5809\n",
      "Epoch 92: loss did not improve from 0.71233\n",
      "61/61 [==============================] - 14s 226ms/step - loss: 0.7157 - accuracy: 0.5809 - lr: 0.0010\n",
      "Epoch 93/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7098 - accuracy: 0.5768\n",
      "Epoch 93: loss improved from 0.71233 to 0.70980, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 276ms/step - loss: 0.7098 - accuracy: 0.5768 - lr: 0.0010\n",
      "Epoch 94/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7106 - accuracy: 0.5819\n",
      "Epoch 94: loss did not improve from 0.70980\n",
      "61/61 [==============================] - 13s 209ms/step - loss: 0.7106 - accuracy: 0.5819 - lr: 0.0010\n",
      "Epoch 95/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7108 - accuracy: 0.5744\n",
      "Epoch 95: loss did not improve from 0.70980\n",
      "61/61 [==============================] - 13s 220ms/step - loss: 0.7108 - accuracy: 0.5744 - lr: 0.0010\n",
      "Epoch 96/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7084 - accuracy: 0.5762\n",
      "Epoch 96: loss improved from 0.70980 to 0.70841, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 234ms/step - loss: 0.7084 - accuracy: 0.5762 - lr: 0.0010\n",
      "Epoch 97/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7057 - accuracy: 0.5765\n",
      "Epoch 97: loss improved from 0.70841 to 0.70575, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 249ms/step - loss: 0.7057 - accuracy: 0.5765 - lr: 0.0010\n",
      "Epoch 98/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7009 - accuracy: 0.5809\n",
      "Epoch 98: loss improved from 0.70575 to 0.70090, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 273ms/step - loss: 0.7009 - accuracy: 0.5809 - lr: 0.0010\n",
      "Epoch 99/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7013 - accuracy: 0.5770\n",
      "Epoch 99: loss did not improve from 0.70090\n",
      "61/61 [==============================] - 13s 217ms/step - loss: 0.7013 - accuracy: 0.5770 - lr: 0.0010\n",
      "Epoch 100/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6999 - accuracy: 0.5773\n",
      "Epoch 100: loss improved from 0.70090 to 0.69991, saving model to nextword1.h5\n",
      "61/61 [==============================] - 16s 266ms/step - loss: 0.6999 - accuracy: 0.5773 - lr: 0.0010\n",
      "Epoch 101/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6996 - accuracy: 0.5786\n",
      "Epoch 101: loss improved from 0.69991 to 0.69955, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 235ms/step - loss: 0.6996 - accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 102/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6985 - accuracy: 0.5726\n",
      "Epoch 102: loss improved from 0.69955 to 0.69852, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 235ms/step - loss: 0.6985 - accuracy: 0.5726 - lr: 0.0010\n",
      "Epoch 103/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6974 - accuracy: 0.5750\n",
      "Epoch 103: loss improved from 0.69852 to 0.69743, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 249ms/step - loss: 0.6974 - accuracy: 0.5750 - lr: 0.0010\n",
      "Epoch 104/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6964 - accuracy: 0.5729\n",
      "Epoch 104: loss improved from 0.69743 to 0.69640, saving model to nextword1.h5\n",
      "61/61 [==============================] - 18s 302ms/step - loss: 0.6964 - accuracy: 0.5729 - lr: 0.0010\n",
      "Epoch 105/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6957 - accuracy: 0.5739\n",
      "Epoch 105: loss improved from 0.69640 to 0.69570, saving model to nextword1.h5\n",
      "61/61 [==============================] - 17s 279ms/step - loss: 0.6957 - accuracy: 0.5739 - lr: 0.0010\n",
      "Epoch 106/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6960 - accuracy: 0.5724\n",
      "Epoch 106: loss did not improve from 0.69570\n",
      "61/61 [==============================] - 12s 197ms/step - loss: 0.6960 - accuracy: 0.5724 - lr: 0.0010\n",
      "Epoch 107/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6927 - accuracy: 0.5775\n",
      "Epoch 107: loss improved from 0.69570 to 0.69268, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 232ms/step - loss: 0.6927 - accuracy: 0.5775 - lr: 0.0010\n",
      "Epoch 108/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6946 - accuracy: 0.5793\n",
      "Epoch 108: loss did not improve from 0.69268\n",
      "61/61 [==============================] - 9s 140ms/step - loss: 0.6946 - accuracy: 0.5793 - lr: 0.0010\n",
      "Epoch 109/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6914 - accuracy: 0.5793\n",
      "Epoch 109: loss improved from 0.69268 to 0.69135, saving model to nextword1.h5\n",
      "61/61 [==============================] - 22s 363ms/step - loss: 0.6914 - accuracy: 0.5793 - lr: 0.0010\n",
      "Epoch 110/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5747\n",
      "Epoch 110: loss did not improve from 0.69135\n",
      "61/61 [==============================] - 15s 216ms/step - loss: 0.6933 - accuracy: 0.5747 - lr: 0.0010\n",
      "Epoch 111/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6921 - accuracy: 0.5786\n",
      "Epoch 111: loss did not improve from 0.69135\n",
      "61/61 [==============================] - 8s 129ms/step - loss: 0.6921 - accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 112/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6885 - accuracy: 0.5747\n",
      "Epoch 112: loss improved from 0.69135 to 0.68850, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 248ms/step - loss: 0.6885 - accuracy: 0.5747 - lr: 0.0010\n",
      "Epoch 113/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6876 - accuracy: 0.5737\n",
      "Epoch 113: loss improved from 0.68850 to 0.68761, saving model to nextword1.h5\n",
      "61/61 [==============================] - 20s 331ms/step - loss: 0.6876 - accuracy: 0.5737 - lr: 0.0010\n",
      "Epoch 114/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6842 - accuracy: 0.5737\n",
      "Epoch 114: loss improved from 0.68761 to 0.68423, saving model to nextword1.h5\n",
      "61/61 [==============================] - 19s 309ms/step - loss: 0.6842 - accuracy: 0.5737 - lr: 0.0010\n",
      "Epoch 115/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6863 - accuracy: 0.5780\n",
      "Epoch 115: loss did not improve from 0.68423\n",
      "61/61 [==============================] - 12s 193ms/step - loss: 0.6863 - accuracy: 0.5780 - lr: 0.0010\n",
      "Epoch 116/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6846 - accuracy: 0.5739\n",
      "Epoch 116: loss did not improve from 0.68423\n",
      "61/61 [==============================] - 11s 187ms/step - loss: 0.6846 - accuracy: 0.5739 - lr: 0.0010\n",
      "Epoch 117/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6845 - accuracy: 0.5729\n",
      "Epoch 117: loss did not improve from 0.68423\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 0.6845 - accuracy: 0.5729 - lr: 0.0010\n",
      "Epoch 118/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6048 - accuracy: 0.6295\n",
      "Epoch 118: loss improved from 0.68423 to 0.60478, saving model to nextword1.h5\n",
      "61/61 [==============================] - 19s 319ms/step - loss: 0.6048 - accuracy: 0.6295 - lr: 2.0000e-04\n",
      "Epoch 119/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5999 - accuracy: 0.6215\n",
      "Epoch 119: loss improved from 0.60478 to 0.59992, saving model to nextword1.h5\n",
      "61/61 [==============================] - 21s 352ms/step - loss: 0.5999 - accuracy: 0.6215 - lr: 2.0000e-04\n",
      "Epoch 120/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5973 - accuracy: 0.6110\n",
      "Epoch 120: loss improved from 0.59992 to 0.59731, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 173ms/step - loss: 0.5973 - accuracy: 0.6110 - lr: 2.0000e-04\n",
      "Epoch 121/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5961 - accuracy: 0.6032\n",
      "Epoch 121: loss improved from 0.59731 to 0.59607, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 174ms/step - loss: 0.5961 - accuracy: 0.6032 - lr: 2.0000e-04\n",
      "Epoch 122/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5959 - accuracy: 0.5989\n",
      "Epoch 122: loss improved from 0.59607 to 0.59588, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 177ms/step - loss: 0.5959 - accuracy: 0.5989 - lr: 2.0000e-04\n",
      "Epoch 123/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5951 - accuracy: 0.5883\n",
      "Epoch 123: loss improved from 0.59588 to 0.59512, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 172ms/step - loss: 0.5951 - accuracy: 0.5883 - lr: 2.0000e-04\n",
      "Epoch 124/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5946 - accuracy: 0.5906\n",
      "Epoch 124: loss improved from 0.59512 to 0.59459, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 174ms/step - loss: 0.5946 - accuracy: 0.5906 - lr: 2.0000e-04\n",
      "Epoch 125/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5940 - accuracy: 0.5822\n",
      "Epoch 125: loss improved from 0.59459 to 0.59400, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 174ms/step - loss: 0.5940 - accuracy: 0.5822 - lr: 2.0000e-04\n",
      "Epoch 126/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5937 - accuracy: 0.5809\n",
      "Epoch 126: loss improved from 0.59400 to 0.59372, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 175ms/step - loss: 0.5937 - accuracy: 0.5809 - lr: 2.0000e-04\n",
      "Epoch 127/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5941 - accuracy: 0.5786\n",
      "Epoch 127: loss did not improve from 0.59372\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.5941 - accuracy: 0.5786 - lr: 2.0000e-04\n",
      "Epoch 128/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5939 - accuracy: 0.5783\n",
      "Epoch 128: loss did not improve from 0.59372\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.5939 - accuracy: 0.5783 - lr: 2.0000e-04\n",
      "Epoch 129/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5937 - accuracy: 0.5755\n",
      "Epoch 129: loss did not improve from 0.59372\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.5937 - accuracy: 0.5755 - lr: 2.0000e-04\n",
      "Epoch 130/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5811 - accuracy: 0.5896\n",
      "Epoch 130: loss improved from 0.59372 to 0.58107, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 181ms/step - loss: 0.5811 - accuracy: 0.5896 - lr: 1.0000e-04\n",
      "Epoch 131/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5809 - accuracy: 0.5793\n",
      "Epoch 131: loss improved from 0.58107 to 0.58092, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 174ms/step - loss: 0.5809 - accuracy: 0.5793 - lr: 1.0000e-04\n",
      "Epoch 132/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5810 - accuracy: 0.5786\n",
      "Epoch 132: loss did not improve from 0.58092\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.5810 - accuracy: 0.5786 - lr: 1.0000e-04\n",
      "Epoch 133/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5809 - accuracy: 0.5801\n",
      "Epoch 133: loss improved from 0.58092 to 0.58089, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 175ms/step - loss: 0.5809 - accuracy: 0.5801 - lr: 1.0000e-04\n",
      "Epoch 134/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5809 - accuracy: 0.5796\n",
      "Epoch 134: loss did not improve from 0.58089\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.5809 - accuracy: 0.5796 - lr: 1.0000e-04\n",
      "Epoch 135/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5808 - accuracy: 0.5719\n",
      "Epoch 135: loss improved from 0.58089 to 0.58078, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 170ms/step - loss: 0.5808 - accuracy: 0.5719 - lr: 1.0000e-04\n",
      "Epoch 136/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5806 - accuracy: 0.5832\n",
      "Epoch 136: loss improved from 0.58078 to 0.58065, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 183ms/step - loss: 0.5806 - accuracy: 0.5832 - lr: 1.0000e-04\n",
      "Epoch 137/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5806 - accuracy: 0.5783\n",
      "Epoch 137: loss improved from 0.58065 to 0.58064, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 180ms/step - loss: 0.5806 - accuracy: 0.5783 - lr: 1.0000e-04\n",
      "Epoch 138/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5806 - accuracy: 0.5721\n",
      "Epoch 138: loss improved from 0.58064 to 0.58062, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 181ms/step - loss: 0.5806 - accuracy: 0.5721 - lr: 1.0000e-04\n",
      "Epoch 139/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5805 - accuracy: 0.5696\n",
      "Epoch 139: loss improved from 0.58062 to 0.58049, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 172ms/step - loss: 0.5805 - accuracy: 0.5696 - lr: 1.0000e-04\n",
      "Epoch 140/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5808 - accuracy: 0.5737\n",
      "Epoch 140: loss did not improve from 0.58049\n",
      "61/61 [==============================] - 8s 129ms/step - loss: 0.5808 - accuracy: 0.5737 - lr: 1.0000e-04\n",
      "Epoch 141/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5807 - accuracy: 0.5768\n",
      "Epoch 141: loss did not improve from 0.58049\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.5807 - accuracy: 0.5768 - lr: 1.0000e-04\n",
      "Epoch 142/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5803 - accuracy: 0.5804\n",
      "Epoch 142: loss improved from 0.58049 to 0.58033, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 176ms/step - loss: 0.5803 - accuracy: 0.5804 - lr: 1.0000e-04\n",
      "Epoch 143/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5807 - accuracy: 0.5765\n",
      "Epoch 143: loss did not improve from 0.58033\n",
      "61/61 [==============================] - 8s 128ms/step - loss: 0.5807 - accuracy: 0.5765 - lr: 1.0000e-04\n",
      "Epoch 144/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5805 - accuracy: 0.5739\n",
      "Epoch 144: loss did not improve from 0.58033\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.5805 - accuracy: 0.5739 - lr: 1.0000e-04\n",
      "Epoch 145/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5803 - accuracy: 0.5726\n",
      "Epoch 145: loss improved from 0.58033 to 0.58026, saving model to nextword1.h5\n",
      "61/61 [==============================] - 8s 139ms/step - loss: 0.5803 - accuracy: 0.5726 - lr: 1.0000e-04\n",
      "Epoch 146/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5804 - accuracy: 0.5726\n",
      "Epoch 146: loss did not improve from 0.58026\n",
      "61/61 [==============================] - 6s 103ms/step - loss: 0.5804 - accuracy: 0.5726 - lr: 1.0000e-04\n",
      "Epoch 147/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5809 - accuracy: 0.5786\n",
      "Epoch 147: loss did not improve from 0.58026\n",
      "61/61 [==============================] - 8s 134ms/step - loss: 0.5809 - accuracy: 0.5786 - lr: 1.0000e-04\n",
      "Epoch 148/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5801 - accuracy: 0.5685\n",
      "Epoch 148: loss improved from 0.58026 to 0.58009, saving model to nextword1.h5\n",
      "61/61 [==============================] - 12s 199ms/step - loss: 0.5801 - accuracy: 0.5685 - lr: 1.0000e-04\n",
      "Epoch 149/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5805 - accuracy: 0.5662\n",
      "Epoch 149: loss did not improve from 0.58009\n",
      "61/61 [==============================] - 6s 103ms/step - loss: 0.5805 - accuracy: 0.5662 - lr: 1.0000e-04\n",
      "Epoch 150/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5804 - accuracy: 0.5752\n",
      "Epoch 150: loss did not improve from 0.58009\n",
      "61/61 [==============================] - 6s 103ms/step - loss: 0.5804 - accuracy: 0.5752 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, epochs=150, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be541707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_21396/2876344870.py:7: UserWarning: Tight layout not applied. The bottom and top margins cannot be made large enough to accommodate all axes decorations.\n",
      "  fig.tight_layout(pad=50.0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the loss and accuracy achieved by the model in each epoch\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(15,10))\n",
    "ax[0].plot(history.history['loss'])\n",
    "ax[1].plot(history.history['accuracy'])\n",
    "ax[0].set_title('model loss')\n",
    "ax[1].set_title('model Accuracy')\n",
    "fig.tight_layout(pad=50.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78011598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
